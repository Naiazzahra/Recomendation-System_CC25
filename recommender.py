# -*- coding: utf-8 -*-
"""recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zJvkfrxT2wvkAk0T4vosHTjAcwjnkDrE

# Import Library

Mengimport library yang akan membantu dalam penyelesaian proyek ini
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score, confusion_matrix
from sklearn.preprocessing import MultiLabelBinarizer,MinMaxScaler, OneHotEncoder
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors
import warnings
from tqdm import tqdm
import re
warnings.filterwarnings('ignore')

from kaggle.api.kaggle_api_extended import KaggleApi

api = KaggleApi()
api.authenticate()

# Download dan unzip otomatis
api.dataset_download_files('arashnic/book-recommendation-dataset',
                           path='dataset',
                           unzip=True)

books = pd.read_csv('dataset/Books.csv')
ratings = pd.read_csv('dataset/Ratings.csv')
users = pd.read_csv('dataset/Users.csv')

print('Jumlah data books: ', len(books.ISBN.unique()))
print('Jumlah data rating unik : ', len(ratings['User-ID'].unique()) + len(ratings['ISBN'].unique()))
print('Jumlah data users : ', len(users['User-ID'].unique()))

books.head()

books.info()

ratings.head()

ratings.info()

users.head()

users.info()

books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce')
valid_years = books['Year-Of-Publication'][
    (books['Year-Of-Publication'] > 1900) & (books['Year-Of-Publication'] <= 2025)
]
books['Year-Of-Publication'].fillna(valid_years.median(), inplace=True)
books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)
books['Book-Author'].fillna('Unknown Author', inplace=True)
books['Publisher'].fillna('Unknown Publisher', inplace=True)
books['Image-URL-L'].fillna('Unknown URL', inplace=True)
print("Info Dataframe setelah cleaning")
books.info()

ratings_explicit = ratings[ratings['Book-Rating'] != 0].copy() # Filter rating eksplisit (1-10)
ratings_explicit.drop_duplicates(subset=['User-ID', 'ISBN'], inplace=True)
print("Info DataFrame 'ratings_explicit' (setelah filter rating 0 & hapus duplikat):")
ratings_explicit.info()

users['Age'] = pd.to_numeric(users['Age'], errors='coerce')
print("Info DataFrame 'users' setelah pembersihan 'Age':")
users.info()

# Penggabungan awal untuk melihat data yang relevan
merged_ratings_books = ratings_explicit.merge(books, on='ISBN', how='inner')
print(f"\nJumlah data gabungan 'ratings_explicit' dan 'books': {len(merged_ratings_books)}")

# Filter Sparsity pada Data Rating
min_ratings_per_user = 5
min_ratings_per_book = 5
user_counts = merged_ratings_books['User-ID'].value_counts()
book_counts = merged_ratings_books['ISBN'].value_counts()
filtered_users = user_counts[user_counts >= min_ratings_per_user].index
filtered_books = book_counts[book_counts >= min_ratings_per_book].index

ratings_filtered = merged_ratings_books[
    (merged_ratings_books['User-ID'].isin(filtered_users)) &
    (merged_ratings_books['ISBN'].isin(filtered_books))
].copy()
print(f"\nJumlah data rating setelah filter sparsity: {len(ratings_filtered)}")
print("\nInfo DataFrame 'ratings_filtered':")
ratings_filtered.info()

plt.figure(figsize=(10, 6))
sns.countplot(x='Book-Rating', data=ratings_explicit, palette='viridis')
plt.title("Distribusi Rating Buku Eksplisit (1-10)", fontsize=16)
plt.xlabel("Rating", fontsize=12)
plt.ylabel("Jumlah Rating", fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.grid(axis='y', alpha=0.75, linestyle='--')
plt.show()

"""Distribusi rating menunjukkan bahwa buku eksplisit umumnya diterima dengan sangat baik oleh pembaca. Rating yang dominan tinggi menunjukkan kepuasan yang tinggi, serta mungkin adanya komunitas pembaca yang loyal. Namun, penting juga mencermati kemungkinan bias dalam pemberian rating, seperti hanya pembaca yang menyukai genre tersebut yang meninggalkan ulasan."""

plt.figure(figsize=(12, 7))
top_authors = books['Book-Author'].value_counts().head(10)
sns.barplot(x=top_authors.values, y=top_authors.index, palette='crest')
plt.title("Top 10 Penulis Paling Produktif", fontsize=16)
plt.xlabel("Jumlah Buku", fontsize=12)
plt.ylabel("Penulis", fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.show()

"""Agatha Christie memimpin sebagai penulis paling produktif, dengan Shakespeare dan King menyusul. Grafik ini menunjukkan bahwa produktivitas luar biasa bisa dicapai oleh penulis dari berbagai era dan genre, dan penulis serial cenderung memiliki volume karya lebih banyak. Informasi ini sangat bermanfaat untuk analisis pasar buku, tren genre, atau inspirasi bagi penulis baru."""

plt.figure(figsize=(12, 7))
top_publishers = books['Publisher'].value_counts().head(10)
sns.barplot(x=top_publishers.values, y=top_publishers.index, palette='magma')
plt.title("Top 10 Penerbit Paling Produktif", fontsize=16)
plt.xlabel("Jumlah Buku", fontsize=12)
plt.ylabel("Penerbit", fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.show()

"""Visualisasi ini menunjukkan bahwa Harlequin adalah penerbit paling produktif, secara signifikan mengungguli yang lain. Sementara itu, sembilan penerbit lainnya memiliki volume yang relatif serupa, menandakan kompetisi yang lebih merata di luar dominasi Harlequin."""

sampled_books = books.dropna(subset=['Book-Title']).sample(n=40000, random_state=42).reset_index(drop=True)

"""Mengambil 40000 sample dari dataset books, karena terlalu banyak 271000 yang menyebabkan selalu crash saat menjalankan program (RAM tidak memenuhi untuk load dataset 271000)"""

sampled_books.head()

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(sampled_books['Book-Title'])

print("Ukuran TF-IDF Matrix:", tfidf_matrix.shape)

cosine_sim = cosine_similarity(tfidf_matrix)

title_to_index = pd.Series(sampled_books.index, index=sampled_books['Book-Title']).drop_duplicates()

def get_similar_books(title, top_n=5):
    try:
        if title not in title_to_index:
            return []

        idx = title_to_index[title]

        # Pastikan idx adalah integer
        if isinstance(idx, pd.Series):  # bisa terjadi jika ada duplikat index
            idx = idx.iloc[0]

        # Ambil vektor similarity dan pastikan hasilnya 1D
        sim_scores = cosine_sim[idx]  # 1D array (misal: shape (40000,))

        # Enumerate dan urutkan berdasarkan skor tertinggi (kecuali dirinya sendiri)
        sim_scores = list(enumerate(sim_scores))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[1:top_n+1]  # skip dirinya sendiri (posisi 0)

        book_indices = [i[0] for i in sim_scores]
        return [sampled_books.iloc[i]['Book-Title'] for i in book_indices]

    except Exception as e:
        print(f"âš ï¸ Error saat merekomendasikan untuk judul: {title}")
        print(e)
        return []

# 7. Evaluasi dengan Precision@5 dan Recall@5
ratings_books_filtered = merged_ratings_books[merged_ratings_books['Book-Title'].isin(sampled_books['Book-Title'])].copy()
user_likes = ratings_books_filtered[ratings_books_filtered['Book-Rating'] >= 8]  # dianggap relevan

# Top 100 pengguna paling aktif
top_users = user_likes['User-ID'].value_counts().head(100).index

precision_list = []
recall_list = []

for user in tqdm(top_users):
    liked_books = user_likes[user_likes['User-ID'] == user]['Book-Title'].unique()
    recommended = set()

    for book in liked_books:
        recommended.update(get_similar_books(book, top_n=5))

    recommended = list(recommended)
    relevant = set(liked_books)

    # Hitung true positives
    true_positives = len(set(recommended) & relevant)
    precision = true_positives / len(recommended) if recommended else 0
    recall = true_positives / len(relevant) if relevant else 0

    precision_list.append(precision)
    recall_list.append(recall)

# Rata-rata
avg_precision = np.mean(precision_list)
avg_recall = np.mean(recall_list)

print(f"\nðŸ“Š Hasil Evaluasi:")
print(f"ðŸ“Œ Precision@5 (rata-rata): {avg_precision:.4f}")
print(f"ðŸ“Œ Recall@5 (rata-rata):    {avg_recall:.4f}")

query = "The F Word"
print(f"\nðŸ“š Rekomendasi untuk buku: '{query}'")
rekomendasi = get_similar_books(query, top_n=5)
for i, book in enumerate(rekomendasi, 1):
    print(f"{i}. {book}")